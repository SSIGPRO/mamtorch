{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mamtorch\n",
    "import random\n",
    "from reference import fullyconnected_reference\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A: torch.Size([4096, 4096])\n",
      "Shape of B: torch.Size([4096, 4096])\n",
      "Shape of bias: torch.Size([4096])\n",
      "Beta value: 0\n",
      "\n",
      "Test kernel v3: fullyconnected\n",
      "Functionality check\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 39.39 GiB of which 31.73 GiB is free. Process 3963148 has 412.00 MiB memory in use. Process 3986332 has 1.71 GiB memory in use. Process 3727046 has 1.89 GiB memory in use. Process 1545816 has 640.00 MiB memory in use. Process 1546705 has 2.21 GiB memory in use. Including non-PyTorch memory, this process has 812.00 MiB memory in use. Of the allocated memory 320.02 MiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunctionality check\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m res, argmax, argmin \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mmamtorch_kernel_v3\u001b[38;5;241m.\u001b[39mfullyconnected(a, b, bias, beta)\n\u001b[0;32m---> 17\u001b[0m res_ref, argmax_ref, argmin_ref \u001b[38;5;241m=\u001b[39m \u001b[43mfullyconnected_reference\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m res_err \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(res\u001b[38;5;241m-\u001b[39mres_ref)\n\u001b[1;32m     19\u001b[0m argmax_err \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(argmax\u001b[38;5;241m-\u001b[39margmax_ref)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m/srv/newpenny/luciano/mamtorch/test/reference.py:4\u001b[0m, in \u001b[0;36mfullyconnected_reference\u001b[0;34m(a, b, bias, beta)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfullyconnected_reference\u001b[39m(a, b, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mb\u001b[49m\n\u001b[1;32m      5\u001b[0m     valmax, argmax \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(v, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      6\u001b[0m     valmin, argmin \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(v, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 39.39 GiB of which 31.73 GiB is free. Process 3963148 has 412.00 MiB memory in use. Process 3986332 has 1.71 GiB memory in use. Process 3727046 has 1.89 GiB memory in use. Process 1545816 has 640.00 MiB memory in use. Process 1546705 has 2.21 GiB memory in use. Including non-PyTorch memory, this process has 812.00 MiB memory in use. Of the allocated memory 320.02 MiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "n, m, l= 1024, 8096, 1024\n",
    "a = torch.randn((n, m), dtype=torch.float32, device=device)\n",
    "b = torch.randn((m, l), dtype=torch.float32, device=device)\n",
    "bias = torch.randn((l,), dtype=torch.float32, device=device)\n",
    "beta = 0#random.uniform(0, 1)\n",
    "print(\"Shape of A:\", a.shape)\n",
    "print(\"Shape of B:\", b.shape)\n",
    "print(\"Shape of bias:\", bias.shape)\n",
    "print(\"Beta value:\", beta)\n",
    "\n",
    "print()\n",
    "print(\"Test kernel v3: fullyconnected\")\n",
    "print(\"Functionality check\")\n",
    "res, argmax, argmin = torch.ops.mamtorch_kernel_v3.fullyconnected(a, b, bias, beta)\n",
    "res_ref, argmax_ref, argmin_ref = fullyconnected_reference(a, b, bias, beta)\n",
    "res_err = torch.abs(res-res_ref)\n",
    "argmax_err = torch.abs(argmax-argmax_ref).to(torch.float32)\n",
    "argmin_err = torch.abs(argmin-argmin_ref).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
